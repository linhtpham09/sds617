{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S&DS 617 Applied Machine Learning and Causal Inference Research Seminar: Assignment 1\n",
    "\n",
    "**Deadline**\n",
    "\n",
    "Assignment 1 is due Monday, February 24th at 1:30pm. Late work will not be accepted. \n",
    "\n",
    "**Submission**\n",
    "\n",
    "Submit your assignment as a .pdf on Gradescope. On Gradescope, there are 2 assignments, one where you will submit a pdf file and one where you will submit the corresponding .ipynb that generated it. \n",
    "Note: The problems in each homework assignment are numbered. When submitting the pdf on Gradescope, please select the correct pages that correspond to each problem. \n",
    "\n",
    "To produce the .pdf, do the following to preserve the cell structure of the notebook:\n",
    "- Go to \"File\" at the top-left of your Jupyter Notebook\n",
    "- Under \"Download as\", select \"HTML (.html)\"\n",
    "- After the .html has downloaded, open it and then select \"File\" and \"Print\"\n",
    "- From the print window, select the option to save as a .pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Comparing BERT vs. GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) In this assignment, we will compare BERT (Bidirectional Encoder Representations from Transformers) with GPT (Generative Pre-training Transformer). Provide detailed explanations of how the architecture, the type of attention mechanism employed, and the approach to tokenization in each model contribute to their respective capabilities and applications. Which model do you think will perform better at sentiment analysis and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'blue'>\n",
    "\n",
    "## Architecture \n",
    "- BERT: At a high level, has four modules: tokenizer, embedding module, encoder, and task head. Though BERT is technically a transformer based model, it is more locally known as an encoder only transformer as it does not generate new text. BERT focuses on constructing latent representations of text, and as a result, cannot generate text. \n",
    "- GPT: GPT is a stack of transformer blocks, each with their own encoder, decoder, self attention and feed forward layers. More colloquially, GPT is considered to be a decoder stack. Its goal is to auto-regressively model the entire corpus rather than understand the representation of the text. \n",
    "\n",
    "## Type of Attention Mechanism Employed\n",
    "- BERT: As BERT is based on the Transformer encoder model, BERT has multi head attention, where there are multiple attention heads. Then, multiple layers of attention are attached in order to create a stack. As a result, it is able to capture input features in sequences (typically sentences) very well. Furthermore, it is bi directional, which means it also accounts for both right and left context for each word. (Hence the \"B\" in BERT)\n",
    "- GPT: GPT also utilizes multi head attention, with several attention layers stacked on top of each other. The main difference is that GPT uses a uni directional attention mechanism which only processes from beginning to end. This allows GPT to generate text and focus on predicting the next word. \n",
    "\n",
    "## Approach of Tokenization \n",
    "- BERT: WordPiece Tokenization only saves the longest subwords that are in a word's vocabulary, and then splits on it. Ex. \"Hugging\" -> \"Hug\" This is helpful for BERT's focus on textual representation as the prefixes and suffixes around a word do not tend to add much to the inherent meaning of the word alone. \n",
    "- GPT: Byte Pair Encoding focuses on the merging rules that come with different combinations of words. \"Hugging\" -> \"Hug\" \"g\" \"ing\" . This is helpful for GPT as GPT is focused on predicting the next word. The suffix can indicate past or future tense, contextual context clues, and any other series of context necessary for correct word generation. \n",
    "\n",
    "## Which model will perform better at sentiment analysis and why? \n",
    "BERT will perform better at sentiment analysis because it inherently focuses on the latent constructions of each word. However, GPT does have strong merit in performing better as it is trained on a larger set of data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) We will now perform sentiment analysis on the IMDb dataset (\"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"). This dataset contains movie reviews along with their associated binary sentiment polarity labels. Code has been provided to you below to train and evaluate BERT. \n",
    "\n",
    "Run the below code to get the test accuracy. Then, modify the code to try getting a higher test accuracy (e.g., adjusting hyperparameters, further model tweaking, data augmentation, etc.). Specify what you modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/accts/ltp8/miniconda/envs/pw/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/accts/ltp8/miniconda/envs/pw/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "2025-02-23 21:08:08.857949: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-23 21:08:08.867872: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740362888.879885 1276790 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740362888.883375 1276790 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-23 21:08:08.895533: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import tarfile\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import openai\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded and extracted to './aclImdb\n"
     ]
    }
   ],
   "source": [
    "# URL of the IMDb dataset\n",
    "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "# Send a GET request to download the content of the dataset\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()  # This will raise an exception if there was a download issue\n",
    "\n",
    "# Open the downloaded content as a file-like object\n",
    "file_like_object = BytesIO(response.content)\n",
    "\n",
    "# Extract the tar.gz file\n",
    "with tarfile.open(fileobj=file_like_object) as tar:\n",
    "    tar.extractall(path=\".\")  # Extract to a directory named aclImdb in the current working directory\n",
    "\n",
    "print(\"Dataset downloaded and extracted to './aclImdb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment\n",
      "0  Zentropa has much in common with The Third Man...       pos\n",
      "1  Zentropa is the most original movie I've seen ...       pos\n",
      "2  Lars Von Trier is never backward in trying out...       pos\n",
      "3  *Contains spoilers due to me having to describ...       pos\n",
      "4  That was the first thing that sprang to mind a...       pos\n",
      "                                              review sentiment\n",
      "0  Previous reviewer Claudio Carvalho gave a much...       pos\n",
      "1  CONTAINS \"SPOILER\" INFORMATION. Watch this dir...       pos\n",
      "2  This is my first Deepa Mehta film. I saw the f...       pos\n",
      "3  This was a great film in every sense of the wo...       pos\n",
      "4  A stunningly well-made film, with exceptional ...       pos\n"
     ]
    }
   ],
   "source": [
    "def load_imdb_dataset(directory):\n",
    "    reviews = []\n",
    "    sentiments = []\n",
    "\n",
    "    for sentiment in [\"pos\", \"neg\"]:\n",
    "        dir_name = os.path.join(directory, sentiment)\n",
    "        for filename in os.listdir(dir_name):\n",
    "            if filename.endswith('.txt'):\n",
    "                with open(os.path.join(dir_name, filename), encoding='utf-8') as file:\n",
    "                    reviews.append(file.read())\n",
    "                    sentiments.append(sentiment)\n",
    "\n",
    "    return pd.DataFrame({'review': reviews, 'sentiment': sentiments})\n",
    "\n",
    "# Load the training dataset\n",
    "dataset_dir = 'aclImdb'\n",
    "df_tr = load_imdb_dataset(os.path.join(dataset_dir, 'train'))\n",
    "\n",
    "# Load the test dataset\n",
    "df_te = load_imdb_dataset(os.path.join(dataset_dir, 'test'))\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df_tr.head())\n",
    "print(df_te.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/accts/ltp8/nltk_data...\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/accts/ltp8/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "\n",
    "# Download necessary NLTK data (only needed once)\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define stop words using NLTK's corpus\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment\n",
      "0  I have seen bad films but this took the p***. ...       neg\n",
      "1  The only other film besides Soylent Green that...       pos\n",
      "2  This is not the stuff of soap-operas but the s...       pos\n",
      "3  I saw this shoot without to know what about we...       pos\n",
      "4  It's not too bad a b complex movie, with Sande...       pos\n"
     ]
    }
   ],
   "source": [
    "#synonym replacement from github \n",
    "def synonym_replacement(words, n):\n",
    "\tnew_words = words.copy()\n",
    "\trandom_word_list = list(set([word for word in words if word not in stop_words]))\n",
    "\trandom.shuffle(random_word_list)\n",
    "\tnum_replaced = 0\n",
    "\tfor random_word in random_word_list:\n",
    "\t\tsynonyms = get_synonyms(random_word)\n",
    "\t\tif len(synonyms) >= 1:\n",
    "\t\t\tsynonym = random.choice(list(synonyms))\n",
    "\t\t\tnew_words = [synonym if word == random_word else word for word in new_words]\n",
    "\t\t\t#print(\"replaced\", random_word, \"with\", synonym)\n",
    "\t\t\tnum_replaced += 1\n",
    "\t\tif num_replaced >= n: #only replace up to n words\n",
    "\t\t\tbreak\n",
    "\n",
    "\t#this is stupid but we need it, trust me\n",
    "\tsentence = ' '.join(new_words)\n",
    "\tnew_words = sentence.split(' ')\n",
    "\n",
    "\treturn new_words\n",
    "\n",
    "def get_synonyms(word):\n",
    "\tsynonyms = set()\n",
    "\tfor syn in wordnet.synsets(word): \n",
    "\t\tfor l in syn.lemmas(): \n",
    "\t\t\tsynonym = l.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
    "\t\t\tsynonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n",
    "\t\t\tsynonyms.add(synonym) \n",
    "\tif word in synonyms:\n",
    "\t\tsynonyms.remove(word)\n",
    "\treturn list(synonyms)\n",
    "\n",
    "def augment_review(review, num_replacements=2):\n",
    "    \"\"\"\n",
    "    Tokenize the review, replace up to num_replacements words with synonyms,\n",
    "    and return the augmented review.\n",
    "    \"\"\"\n",
    "    words = review.split()\n",
    "    new_words = synonym_replacement(words, num_replacements)\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "half_size = len(df_te) // 2\n",
    "\n",
    "# Randomly select indices to augment (half of them)\n",
    "indices_to_augment = random.sample(df_te.index.tolist(), half_size)\n",
    "\n",
    "# Apply synonym replacement to reviews at the selected indices\n",
    "# You can adjust num_replacements as needed (e.g., 2 words replaced)\n",
    "df_te.loc[indices_to_augment, 'review'] = df_te.loc[indices_to_augment, 'review'].apply(\n",
    "    lambda review: augment_review(review, num_replacements=2)\n",
    ")\n",
    "df_te = df_te.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "half_size = len(df_tr) // 2\n",
    "\n",
    "# Randomly select indices to augment (half of them)\n",
    "indices_to_augment = random.sample(df_tr.index.tolist(), half_size)\n",
    "\n",
    "df_tr.loc[indices_to_augment, 'review'] = df_tr.loc[indices_to_augment, 'review'].apply(\n",
    "    lambda review: augment_review(review, num_replacements=2)\n",
    ")\n",
    "df_tr = df_tr.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n",
      "(500, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'... Once. \"Manos, the Hands of Fate.\" That was worse than this, quite a bit worse: but it did have one thing: it had beautiful women in negligees wresting each other -- for about 20 minutes. This has a fat 45 year-old with 3 tits and a tail, in a cantina scene cloned directly from \"Star Wars.\" Not to mention an obese, blue seductress Uhura, her fat legs and ass hanging out of some sort of insane bird costume, in this Method Acting Mess. She always wanted to perform before a \"captive audience\"? She must have meant the poor slobs who shelled out 8 bucks hoping to see another \"Wrath of Khan,\" or at least a \"Voyage Home.\" Captive\" is right. I wonder how many people in the theaters tried to slit their wrists while crying out: \"mother, make it stop.\"<br /><br />No question about it, \"Final Frontier\" is not just an unmitigated disaster, it\\'s cruel and unusual punishment. This is Star Trek from hell. This is Shatner on mushrooms -- or maybe peyote. This is Where No Man Has Gone Before and Wished He Never Had in the First Place. Or, to paraphrase a review of \"Heaven\\'s Gate: \"It\\'s as if Gene Roddenberry sold his soul to the Devil for the success of a TV series, and Devil is just now coming around to collect.\"<br /><br />And don\\'t even get me started on a drunken Kirk and a grinning McCoy singing \"Row, Row, Row Your Boat\" together, like they were lovers in some sort of demented gay fever dream. Then we\\'ve got the Hideous Dynamic Duo of Sulu and Chekov, hiking through the woods together... probably en route to a Barry Manilow concert. Then there\\'s Laurence Luckinbill as Spock\\'s brother???!!! Yeah, right! Amazing how these relations we never heard of suddenly crawl out of the woodwork when we need a new plot line. And not to forget Spock rocketing through the air after Kirk when he falls from a cliff in Yosemite. Sure. He catches up to Kirk and saves him ONE FOOT away from the ground. Where\\'d you get those nifty Rocket Shoes, Spock?!'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Subsample train and test sets down (note: you may change the size of training) \n",
    "df_tr = df_tr.sample(n=1000, random_state=928)\n",
    "print(df_tr.shape) # check dimensions\n",
    "df_te = df_te.sample(n=500, random_state=2755)\n",
    "print(df_te.shape) # check dimensions\n",
    "df_te.iloc[1, 0] # sample movie review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDbDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train BERT (Note: this may take a considerable amount of time. You may modify the size of training if too computationally intensive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Function to tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Load the dataset (assuming df_tr is your loaded DataFrame)\n",
    "texts = df_tr['review'].tolist()\n",
    "labels = df_tr['sentiment'].apply(lambda x: 1 if x == 'pos' else 0).tolist()\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the data\n",
    "ml = 150 #original 128 \n",
    "tokenized_dataset = tokenizer(texts, padding=True, truncation=True, max_length=ml)\n",
    "\n",
    "# Splitting the dataset into training and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_texts, val_texts, train_masks, val_masks, train_labels, val_labels = train_test_split(\n",
    "    tokenized_dataset['input_ids'], tokenized_dataset['attention_mask'], labels, test_size=0.2\n",
    ")\n",
    "\n",
    "# Creating dataset objects for training and validation\n",
    "class IMDbDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = IMDbDataset({'input_ids': train_texts, 'attention_mask': train_masks}, train_labels)\n",
    "val_dataset = IMDbDataset({'input_ids': val_texts, 'attention_mask': val_masks}, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/accts/ltp8/miniconda/envs/pw/lib/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 03:04, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.694600</td>\n",
       "      <td>0.695629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.612200</td>\n",
       "      <td>0.598605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.462500</td>\n",
       "      <td>0.477729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.323100</td>\n",
       "      <td>0.462215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.202500</td>\n",
       "      <td>0.529964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.045200</td>\n",
       "      <td>0.856025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.095400</td>\n",
       "      <td>0.819184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>1.084927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.018400</td>\n",
       "      <td>1.102773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.124900</td>\n",
       "      <td>0.929620</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=0.27394897166267035, metrics={'train_runtime': 184.7931, 'train_samples_per_second': 43.292, 'train_steps_per_second': 2.706, 'total_flos': 616666536000000.0, 'train_loss': 0.27394897166267035, 'epoch': 10.0})"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load BERT model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.001,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",  # Ensure models are saved at each epoch\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate at each epoch\n",
    "    optim=\"adamw_torch\",  # Use the recommended optimizer\n",
    ")\n",
    "\n",
    "\n",
    "# Define the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set\n",
    "predictions = trainer.predict(val_dataset)\n",
    "val_accuracy = accuracy_score(val_labels, predictions.predictions.argmax(-1))\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = df_te['review'].tolist()\n",
    "test_labels = df_te['sentiment'].apply(lambda x: 1 if x == 'pos' else 0).tolist()\n",
    "\n",
    "# Tokenize the test data\n",
    "test_encodings = tokenizer(test_texts, padding=True, truncation=True, max_length=ml)\n",
    "\n",
    "class IMDbDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = {key: torch.tensor(val) for key, val in encodings.items()}\n",
    "        self.labels = torch.tensor(labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]  \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "test_dataset = IMDbDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.84\n"
     ]
    }
   ],
   "source": [
    "# Predictions\n",
    "test_predictions = trainer.predict(test_dataset)\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions.predictions.argmax(-1))\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'blue'>\n",
    "\n",
    "*Run the below code to get the test accuracy. Then, modify the code to try getting a higher test accuracy (e.g., adjusting hyperparameters, further model tweaking, data augmentation, etc.). Specify what you modified.* \n",
    "\n",
    "The original validation accuracy was 0.83 and the original test accuracy was .802. \n",
    "\n",
    "First, I slowly increased epoch sizes. With an epoch of 4, my accuracies were (.815, .822). I decided to try and increase my batch size to 16 which gave me the accuracies (.826, .796). Because of this, I decreased my batch size to 10 and also my weight decay to .001, which changed my scores to (.825, .818). I decided to go down one epoch to see if there was any large difference, but there was not. I then tried to increase my weight decay back to .01, and got (.85, .828). \n",
    "\n",
    "Then, I tried to change the token size from 128 to 150. However, this only decreased my test accuracy down to .818. (216 slowed down my computer a lot, and I decided it was not worth it to try.)\n",
    "\n",
    "I realized at this point I could do data augmentation, so I this open source code: https://github.com/jasonwei20/eda_nlp/tree/master for synonym replacement. My initial results with this replacement and the default settings gave me accuracy results fo (.78, .82), which were not incredibly great. \n",
    "\n",
    "Lastly, I decided to try and follow the rule of thumb with experiments like this, where there are generally better results with an increase in batch size, a decrease in learning rate/weight decay, and an increase in epoch size. With this, I received my best results at .84. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Perform sentiment analysis using GPT-3.5-turbo, gpt-4o, o1-mini, and o3-mini and get the test accuracy. Evaluate their performance by comparing test accuracies. (If you get a rate limit error, just use 4o)\n",
    "\n",
    "**Note: DO NOT try to run advanced models on the entire test set initially.** Be mindful of API usage limits and costs associated with the advanced models APIs. Start with a smaller subset of your test set to ensure your implementation is correct before scaling up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from dotenv import load_dotenv\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the OpenAI API key\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Use the API key\n",
    "if openai_api_key:\n",
    "    print(\"OpenAI API Key loaded successfully!\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not found. Please check your .env file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up the OpenAI client\n",
    "client = openai.OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With BERT, we had to pass in tokenized input, but with OpenAI we can pass in raw text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Dataset - for reference\n",
    "test_texts = df_te['review'].tolist()\n",
    "test_labels = df_te['sentiment'].apply(lambda x: 1 if x == 'pos' else 0).tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT 3.5 Turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on:  gpt-3.5-turbo\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_type = \"gpt-3.5-turbo\"\n",
    "\n",
    "\n",
    "def sentiment(txt): \n",
    "    prompt = (\n",
    "        \"Determine the sentiment of the given text.\\n\"\n",
    "        \"Answer only with 'positive' or 'negative'\\n\"\n",
    "        f\"Review: \\\"{txt}\\\"\"\n",
    "    )\n",
    "    # Make a chat completion request\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a sentiment analysis assistant.\"}, \n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        model=model_type,  # Specify the model\n",
    "        temperature = 0, \n",
    "        max_tokens = 2\n",
    "    )\n",
    "    sentiment = chat_completion.choices[0].message.content.strip().lower()\n",
    "    return 1 if sentiment == 'positive' else 0 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on:  gpt-3.5-turbo\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing on: \", model_type)\n",
    "gpt_results = df_te['review'].apply(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test results for  gpt-3.5-turbo :  0.938\n"
     ]
    }
   ],
   "source": [
    "acc_results = accuracy_score(test_labels, gpt_results)\n",
    "print(\"Test results for \", model_type, \": \", acc_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-4o "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on:  gpt-4o\n",
      "Test results for  gpt-4o :  0.946\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_type = \"gpt-4o\"\n",
    "\n",
    "def sentiment(txt): \n",
    "    prompt = (\n",
    "        \"Determine the sentiment of the given text.\\n\"\n",
    "        \"Answer only with 'positive' or 'negative'\\n\"\n",
    "        f\"Review: \\\"{txt}\\\"\"\n",
    "    )\n",
    "    # Make a chat completion request\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a sentiment analysis assistant.\"}, \n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        model=model_type,  # Specify the model\n",
    "        temperature = 0, \n",
    "        max_tokens = 2\n",
    "    )\n",
    "    sentiment = chat_completion.choices[0].message.content.strip().lower()\n",
    "    return 1 if sentiment == 'positive' else 0 \n",
    "\n",
    "\n",
    "print(\"Testing on: \", model_type)\n",
    "gpt_results = df_te['review'].apply(sentiment)\n",
    "\n",
    "acc_results = accuracy_score(test_labels, gpt_results)\n",
    "print(\"Test results for \", model_type, \": \", acc_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT o1- mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on:  o1-mini\n",
      "Test results for  o1-mini :  0.522\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_type = \"o1-mini\"\n",
    "\n",
    "def sentiment(txt): \n",
    "    prompt = (\n",
    "        \"Determine the sentiment of the given text.\\n\"\n",
    "        \"Answer only with 'positive' or 'negative'.\\n\"\n",
    "        f\"Review: \\\"{txt}\\\"\"\n",
    "    )\n",
    "    # Make a chat completion request without a system message\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        model=model_type,  # Specify the model\n",
    "        temperature=1, \n",
    "        max_completion_tokens =2 \n",
    "    )\n",
    "    sentiment_response = chat_completion.choices[0].message.content.strip().lower()\n",
    "    return 1 if sentiment_response == 'positive' else 0 \n",
    "\n",
    "\n",
    "print(\"Testing on: \", model_type)\n",
    "gpt_results = df_te['review'].apply(sentiment)\n",
    "\n",
    "acc_results = accuracy_score(test_labels, gpt_results)\n",
    "print(\"Test results for \", model_type, \": \", acc_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT o3-mini "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on:  o3-mini\n",
      "Test results for  o3-mini :  0.522\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_type = \"o3-mini\"\n",
    "\n",
    "def sentiment(txt): \n",
    "    prompt = (\n",
    "        \"Determine the sentiment of the given text.\\n\"\n",
    "        \"Answer only with 'positive' or 'negative'\\n\"\n",
    "        f\"Review: \\\"{txt}\\\"\"\n",
    "    )\n",
    "    # Make a chat completion request\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a sentiment analysis assistant.\"}, \n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        model=model_type,  # Specify the model\n",
    "        temperature = 1, \n",
    "        max_completion_tokens = 3\n",
    "    )\n",
    "    sentiment = chat_completion.choices[0].message.content.strip().lower()\n",
    "    return 1 if sentiment == 'positive' else 0 \n",
    "\n",
    "\n",
    "\n",
    "print(\"Testing on: \", model_type)\n",
    "gpt_results = df_te['review'].apply(sentiment)\n",
    "\n",
    "acc_results = accuracy_score(test_labels, gpt_results)\n",
    "print(\"Test results for \", model_type, \": \", acc_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'blue'>\n",
    "\n",
    "Results: \n",
    "\n",
    "- 3.5 Turbo: 0.938 \n",
    "- 4o: 0.946 \n",
    "- o1-mini: 0.522 \n",
    "- o3-mini: 0.522 \n",
    "\n",
    "The accuracy results are cleanly split between GPT 4o/GPT 3.5 Turbo and o1-mini/o3-mini. These results generally make sense as o1-mini and o3-mini are smaller in size than 4o/3.5 Turbo and are generally expected to perform worse than their bigger counter parts. Upon research, o1-mini is meant to specifically excel in STEM related subjects such as math and coding. This explains why o1-mini has a harder time on sentient analysis and is mostly likely randomnly guessing between positive/negative to achieve an accuracy rate of 0.522. o3-mini is also STEM focused, focusing on logistical challenges rather than the sentiment nuances of languages. 3.5 Turbo and 4o are large models that have been trained on a wide variety of different corpuses and problem solving. As a result, it makes sense these models are more equipped to handle this specific task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) For the task of language translation, do you expect BERT or GPT to perform better? Explain why in detail. Additionally, discuss the primary challenges associated with implementing each model for translation tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'blue'>\n",
    "For the task of language translation, I expect GPT to perform \"better\". While this is primarily because GPT has been generally trained on more data than BERT, it is also because GPT can provide multiple different translations and nuances to provide a sufficient answer. Unlike sentiment analysis, language translation metrics have relied on evaluations like the FLEURs score, but even then, translation is dynamic and nuanced. While it seems like BERT would be able to build construct better meanings and understandings thanks to its bidirectional attention mechanism, language translation is kind of like language generation. We are not trying to construct a new answer based on the original prompt in language one, we are trying to completely generate a new answer in language two. In that understanding, I understand language translation as a form of prompt generation because there are multiple translations for the word \"good\" in any language. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
