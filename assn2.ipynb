{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S&DS 617 Applied Machine Learning and Causal Inference Research Seminar: Assignment 2\n",
    "\n",
    "**Deadline**\n",
    "\n",
    "Assignment 2 is due Monday, March 24th at 1:30pm. Late work will not be accepted.\n",
    "\n",
    "\n",
    "**Submission**\n",
    "\n",
    "Submit your assignment as a .pdf on Gradescope. On Gradescope, there are 2 assignments, one where you will submit a pdf file and one where you will submit the corresponding .ipynb that generated it. \n",
    "Note: The problems in each homework assignment are numbered.When submitting the pdf on Gradescope, please select the correct pages that correspond to each problem. \n",
    "\n",
    "To produce the .pdf, do the following to preserve the cell structure of the notebook:\n",
    "- Go to \"File\" at the top-left of your Jupyter Notebook\n",
    "- Under \"Download as\", select \"HTML (.html)\"\n",
    "- After the .html has downloaded, open it and then select \"File\" and \"Print\"\n",
    "- From the print window, select the option to save as a .pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "In this exercise, we'll employ different prompt tuning techniques on GSM8k dataset [Link](https://github.com/openai/grade-school-math/tree/master/grade_school_math/data).\n",
    "\n",
    "The GSM8K dataset is an OpenAI-curated collection of 8,500 grade school math problems designed to challenge and evaluate the mathematical reasoning capabilities of language models. It contains approximately 7,500 training and 1,000 test problems that require 2 to 8 steps to solve, using basic arithmetic operations. The dataset aims to diagnose current model limitations in multi-step reasoning and supports advancements in AI's understanding and processing of natural language math problems. It includes both standard problems and a Socratic format with guiding subquestions, along with calculation annotations to aid in accuracy, making it a valuable resource for AI research in natural language processing.\n",
    "\n",
    "Below, we have provided helper functions to load the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import tarfile\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import openai\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "import ast  \n",
    "import sys\n",
    "import time\n",
    "import concurrent.futures\n",
    "\n",
    "# Function to load data from a URL\n",
    "def load_data_from_url(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        data = [json.loads(line) for line in response.iter_lines(decode_unicode=True)]\n",
    "        df = pd.DataFrame(data)\n",
    "        return df\n",
    "    else:\n",
    "        print(f\"Failed to download the file: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Function to get the true solution from JSON file \n",
    "def extract_solution(problem):\n",
    "    \"\"\"\n",
    "    Extracts the final numeric solution from a problem dictionary with 'question' and 'answer' keys.\n",
    "    The answer is expected to contain a '####' token followed by the final numeric solution.\n",
    "    \n",
    "    :param problem: A dictionary with 'question' and 'answer' keys.\n",
    "    :return: The final numeric solution as a string.\n",
    "    \"\"\"\n",
    "    # Split the answer into lines\n",
    "    answer_lines = problem['answer'].split('\\n')\n",
    "    # Look for the line with the '####' token\n",
    "    for line in answer_lines:\n",
    "        if '####' in line:\n",
    "            # Extract the numeric solution that follows the '####' token\n",
    "            solution = line.split('####')[-1].strip()\n",
    "            return solution\n",
    "    # If no solution is found, return None\n",
    "    return None\n",
    "\n",
    "def get_answer(string):\n",
    "    \"\"\"\n",
    "    Extracts the final numeric solution from a string.\n",
    "    The answer is expected to contain a '####' token followed by the final numeric solution.\n",
    "\n",
    "    :param string: a string with the answer \n",
    "    :return: The final numeric solution as a string, or None.\n",
    "    \"\"\"\n",
    "    if '####' in string:\n",
    "        solution = string.split('####')[-1].strip()\n",
    "        sol = re.search(r\"[-+]?\\d[\\d,]*(\\.\\d+)?\", solution)\n",
    "        if sol:\n",
    "            cleaned = sol.group().replace(',', '')\n",
    "            return cleaned\n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 500)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# URLs for the train and test data\n",
    "train_url = \"https://raw.githubusercontent.com/openai/grade-school-math/master/grade_school_math/data/train.jsonl\"\n",
    "test_url = \"https://raw.githubusercontent.com/openai/grade-school-math/master/grade_school_math/data/test.jsonl\"\n",
    "\n",
    "# Load the training data\n",
    "df_train = load_data_from_url(train_url)\n",
    "df_train = df_train.sample(n=1000, random_state=928)\n",
    "\n",
    "# Load the test data\n",
    "df_test = load_data_from_url(test_url)\n",
    "df_test = df_test.sample(n=500, random_state=184)\n",
    "labels = df_test.apply(extract_solution, axis = 1)\n",
    "# Display the lengths of the datasets as a check\n",
    "len(df_train), len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question    Taegan goes to a carnival where she wins ticke...\n",
      "answer      If tickets are valued at $3 then in total, Tae...\n",
      "Name: 3103, dtype: object\n"
     ]
    }
   ],
   "source": [
    "sample_row = df_train.iloc[0,] \n",
    "print(sample_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Taegan goes to a carnival where she wins tickets from each of the 5 carnival games and also finds 5 tickets on the floor. Each ticket is worth $3. In total, she has tickets that total a value of $30. If Taegan won an equal number of tickets from each of the games, how many tickets did she win from each game?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_row['question'] # sample question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the solution\n",
    "extract_solution(sample_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from dotenv import load_dotenv\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the OpenAI API key\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Use the API key\n",
    "if openai_api_key:\n",
    "    print(\"OpenAI API Key loaded successfully!\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not found. Please check your .env file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up the OpenAI client\n",
    "client = openai.OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Implement zero shot learning, few shot learning, and chain of thought prompting using gpt-4o. Make a figure or table comparing the accuracy of each on the test set and comment on your results and whether they align with your expectations. Sample at most 1000 observations. Again, remember to start with a smaller subset of your dataset to ensure your implementation is correct before scaling up. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero Shot 4o-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_shot(txt): \n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    \"Solve the following math word problem. \"\n",
    "                    \"Only include one integer at the end of your response, and put it after '####' on a new line.\\n\\n\"\n",
    "                    f\"{txt}\\n\\n\"\n",
    "                )\n",
    "            }\n",
    "        ],\n",
    "        model=model_type\n",
    "    )\n",
    "    response = chat_completion.choices[0].message.content\n",
    "    res = get_answer(response)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_results = []\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:  # 5 parallel threads\n",
    "    gpt_results = list(executor.map(zero_shot, df_test['question']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test results for  gpt-4o-mini  zero-shot:  0.95\n"
     ]
    }
   ],
   "source": [
    "acc_results = accuracy_score(labels, gpt_results)\n",
    "print(\"Test results for \", model_type, \" zero-shot: \", acc_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot 4o-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(txt, model_type, prompt):\n",
    "    def call(txt): \n",
    "        chat_completion = client.chat.completions.create(\n",
    "                messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    \n",
    "                    \"Solve the following math word problem. \"\n",
    "                    \"Only include one integer at the end of your response, and put it after '####' on a new line.\"\n",
    "                    f\"Here are a few examples: {prompt}\"\n",
    "                    f\"These are the questions: {txt}\\n\\n\"\n",
    "                )\n",
    "            }\n",
    "            ],\n",
    "                model=model_type# Specify the model\n",
    "            )\n",
    "        response = chat_completion.choices[0].message.content\n",
    "        res = get_answer(response)\n",
    "        return res\n",
    "    gpt_results = []\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:  # 5 parallel threads\n",
    "        gpt_results = list(executor.map(call, txt))\n",
    "\n",
    "    return gpt_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"gpt-4o-mini\"\n",
    "few_shot_prompt = \"\"\"\n",
    "Q: What is 10*5? \n",
    "#### 50 \n",
    "\n",
    "Q: How many miles did Rob drive if he drove for 3 hours at 20 miles per hour? \n",
    "#### 60 \n",
    "\"\"\"\n",
    "\n",
    "gpt_results = run_experiment(df_test['question'], model_type, few_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test results for  gpt-4o-mini  few-shot:  0.942\n"
     ]
    }
   ],
   "source": [
    "acc_results = accuracy_score(labels, gpt_results)\n",
    "print(\"Test results for \", model_type, \" few-shot: \", acc_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain of Thought 4o-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"gpt-4o-mini\"\n",
    "cot_prompt = \"\"\"\n",
    "Q: Olivia has 23 books. She buys 18 more books. Then she gives 15 books to her friends. How many books does she have now?\n",
    "A: Let's break it down step by step.\n",
    "- Olivia starts with 23 books.\n",
    "- She buys 18 more: 23 + 18 = 41.\n",
    "- She gives away 15: 41 - 15 = 26.\n",
    "#### 26\n",
    "\n",
    "Q: A box holds 12 pencils. A teacher has 7 boxes. She gives 10 pencils to each of 3 students. How many pencils does she have left?\n",
    "A: Step by step:\n",
    "- Total pencils = 12 × 7 = 84.\n",
    "- 3 students each get 10 pencils \n",
    "→ 3 × 10 = 30.\n",
    "- Pencils left = 84 - 30 = 54.\n",
    "#### 54\n",
    "\"\"\"\n",
    "gpt_results = run_experiment(df_test['question'], model_type, cot_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test results for  gpt-4o-mini cot:  0.94\n"
     ]
    }
   ],
   "source": [
    "acc_results = accuracy_score(labels, gpt_results)\n",
    "print(\"Test results for \", model_type, \"cot: \", acc_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a) Final Results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'blue'>\n",
    "\n",
    "| Model   | Prompting Method | Accuracy |\n",
    "|---------|----------------|----------|\n",
    "| 4o-mini | Zero-Shot      |  0.95   |\n",
    "| 4o-mini | Few-Shot       | 0.942  |\n",
    "| 4o-mini | Chain of Thought       |  0.94|\n",
    "\n",
    "\n",
    "These results suprise me as I expected Few Shot and Chain of Thought to have higher accuracy than Zero shot. However, this may be because 4o-mini was trained through zero shot, and the added tokens forced it to create more incorrect answers. I am specifically referring to the variable compute section of the Wei paper, where more tokens did not necessarily mean higher accuracy. This may mean that 4o-mini works best with little to no guidance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b)  Explore recent literature for reasoning methods that could enhance the performance of CoT and improve the baseline obtained in a). Then, compare the results with the reasoning models o1 and o3, and discuss your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## o1-mini Few Shot and CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test results for  o1-mini cot:  0.964\n"
     ]
    }
   ],
   "source": [
    "model_type = \"o1-mini\"\n",
    "gpt_results = run_experiment(df_test['question'], model_type, few_shot_prompt)\n",
    "acc_results = accuracy_score(labels, gpt_results)\n",
    "print(\"Test results for \", model_type, \"cot: \", acc_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test results for  o1-mini cot:  0.956\n"
     ]
    }
   ],
   "source": [
    "model_type = \"o1-mini\"\n",
    "gpt_results = run_experiment(df_test['question'], model_type, cot_prompt)\n",
    "acc_results = accuracy_score(labels, gpt_results)\n",
    "print(\"Test results for \", model_type, \"cot: \", acc_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## o3-mini Few Shot and CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test results for  o3-mini cot:  0.958\n"
     ]
    }
   ],
   "source": [
    "model_type = \"o3-mini\"\n",
    "gpt_results = run_experiment(df_test['question'], model_type, few_shot_prompt)\n",
    "acc_results = accuracy_score(labels, gpt_results)\n",
    "print(\"Test results for \", model_type, \"cot: \", acc_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test results for  o3-mini cot:  0.968\n"
     ]
    }
   ],
   "source": [
    "model_type = \"o3-mini\"\n",
    "gpt_results = run_experiment(df_test['question'], model_type, cot_prompt)\n",
    "acc_results = accuracy_score(labels, gpt_results)\n",
    "print(\"Test results for \", model_type, \"cot: \", acc_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'blue'>\n",
    "\n",
    "# Tree of Thought\n",
    "\n",
    "In [Yao 2023](https://arxiv.org/abs/2305.10601v2) they discuss Tree of Thought as a reasoning method to improve Chain of Thought. Thus I decided to try it and see if it would improve accuracy results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tree_prompt = \"\"\"\n",
    "Q: Emma has $50. She buys 2 books for $12 each and 1 backpack for $20. How much money does she have left?\n",
    "\n",
    "A: Let’s explore different solution paths:\n",
    "\n",
    "Option 1:\n",
    "\n",
    "Cost of books = 2 × 12 = 24\n",
    "Total cost = 24 + 20 = 44\n",
    "Leftover = 50 − 44 = 6\n",
    "Option 2:\n",
    "\n",
    "Buy backpack first: 50 − 20 = 30\n",
    "Then books: 30 − (2 × 12) = 30 − 24 = 6\n",
    "Option 3:\n",
    "\n",
    "Add costs: 12 + 12 + 20 = 44\n",
    "Leftover = 50 − 44 = 6\n",
    "All paths give the same result. Final answer:\n",
    "\n",
    "#### 6\n",
    "\n",
    "Q: A pizza is cut into 8 slices. Tom eats 2 slices, Jerry eats 3, and Spike eats the rest. How many slices did Spike eat?\n",
    "\n",
    "A: Consider several ways to reason through it:\n",
    "\n",
    "Path A:\n",
    "\n",
    "Tom + Jerry = 2 + 3 = 5\n",
    "Spike = 8 − 5 = 3\n",
    "Path B:\n",
    "\n",
    "Count what’s left: Only 3 slices not mentioned, must be Spike’s.\n",
    "Path C:\n",
    "\n",
    "8 total. Deduct Tom’s 2 = 6. Deduct Jerry’s 3 = 3.\n",
    "All paths agree.\n",
    "\n",
    "#### 3\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete!\n",
      "Test results for  o1-mini tree:  0.958\n"
     ]
    }
   ],
   "source": [
    "model_type = \"o1-mini\"\n",
    "gpt_results = run_experiment(df_test['question'], model_type,tree_prompt)\n",
    "print(\"Processing complete!\")\n",
    "acc_results = accuracy_score(labels, gpt_results)\n",
    "print(\"Test results for \", model_type, \"tree: \", acc_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete!\n",
      "Test results for  o3-mini tree:  0.962\n"
     ]
    }
   ],
   "source": [
    "model_type = \"o3-mini\"\n",
    "gpt_results = run_experiment(df_test['question'], model_type,tree_prompt)\n",
    "print(\"Processing complete!\")\n",
    "acc_results = accuracy_score(labels, gpt_results)\n",
    "print(\"Test results for \", model_type, \"tree: \", acc_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete!\n",
      "Test results for  gpt-4o-mini tree:  0.926\n"
     ]
    }
   ],
   "source": [
    "model_type = \"gpt-4o-mini\"\n",
    "gpt_results = run_experiment(df_test['question'], model_type,tree_prompt)\n",
    "print(\"Processing complete!\")\n",
    "acc_results = accuracy_score(labels, gpt_results)\n",
    "print(\"Test results for \", model_type, \"tree: \", acc_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'blue'>\n",
    "\n",
    "# b) Final Results\n",
    "\n",
    "| Model   | Prompting Method | Accuracy |\n",
    "|---------|----------------|----------|\n",
    "| o1-mini | few shot|  0.964|\n",
    "| o3-mini | few shot | 0.958|\n",
    "| o1-mini |CoT |0.956|\n",
    "| o3-mini |  CoT  |0.968|\n",
    "| 4o-mini |  CoT  |0.94|\n",
    "| o1-mini |  Tree  |0.958|\n",
    "| o3-mini |  Tree  |0.962|\n",
    "| 4o-mini |  Tree  |0.926|\n",
    "\n",
    "\n",
    "These results suprise me as I always assume the most recent model of GPT to be \"the best.\" However, now I understand that the type of prompt has a large impact on performance. It appears that 4o-mini seems to have problems with long guided prompts, and it works best when left as zero shot. However, o3-mini acts in reverse and prefers longer guided prompts. Perhaps it was trained to expect more guidance. o1-mini exists in the middle. This is important to understand as understand the type of prompt will produce different results for each model, and so I should adapt my methods according to the model I choose. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Perform an ablation study similar to that of Section 3.3 of Wei et al. 2023: [Link](https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf). Comment on your results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Wei et al. 2023, they discuss how chain of thought may be impacted by the amount of tokens present in the prompt. I want to test whether a sparser answers focused only on numbers would produce better results. This is also called symbolic reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete!\n",
      "Test results for  o1-mini symbolic reasoning:  0.96\n"
     ]
    }
   ],
   "source": [
    "model_type = \"o1-mini\"\n",
    "sparse_prompt = \"\"\"\n",
    "Q: Emily has 12 pencils. She gives 3 pencils to each of her 4 friends. How many pencils does she have left?\n",
    "A: (3 * 4) - 12 = 0\n",
    "#### 0\n",
    "\n",
    "Q: A pack of gum costs $3. You buy 4 packs and a soda that costs $2. How much do you spend?\n",
    "A: 4 * 3 + 2 = 14 \n",
    "#### 14 \n",
    "\n",
    "Q: Solve for x: 3x + 4 = 19\n",
    "A: (19 − 4)/3 = 5\n",
    "##### 14 \n",
    "\"\"\"\n",
    "\n",
    "gpt_results = run_experiment(df_test['question'], model_type,sparse_prompt)\n",
    "\n",
    "print(\"Processing complete!\")\n",
    "acc_results = accuracy_score(labels, gpt_results)\n",
    "print(\"Test results for \", model_type, \"symbolic reasoning: \", acc_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete!\n",
      "Test results for  o3-mini symbolic reasoning:  0.966\n"
     ]
    }
   ],
   "source": [
    "model_type = \"o3-mini\"\n",
    "gpt_results = run_experiment(df_test['question'], model_type,sparse_prompt)\n",
    "\n",
    "print(\"Processing complete!\")\n",
    "acc_results = accuracy_score(labels, gpt_results)\n",
    "print(\"Test results for \", model_type, \"symbolic reasoning: \", acc_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete!\n",
      "Test results for  gpt-4o-mini symbolic reasoning:  0.916\n"
     ]
    }
   ],
   "source": [
    "model_type = \"gpt-4o-mini\"\n",
    "gpt_results = run_experiment(df_test['question'], model_type,sparse_prompt)\n",
    "\n",
    "print(\"Processing complete!\")\n",
    "acc_results = accuracy_score(labels, gpt_results)\n",
    "print(\"Test results for \", model_type, \"symbolic reasoning: \", acc_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "\n",
    "# c) Final results\n",
    "| Model   |  Accuracy |\n",
    "|---------|----------|\n",
    "| o1-mini | 0.96|\n",
    "| o3-mini | 0.966|\n",
    "| 4o-mini |  0.916|\n",
    "\n",
    "Because of the prior experiments, I was interested in performing an Ablation study to examine the impact of sparser yet still guided prompts. I wanted to provide a bare bones, numerical only construction, which according to GPT is calle \"symbolic reasoning.\" We can see the o3-mini did extremely well with symbolic reasoning, and it makes me reconsider my earlier statements about prompt length. I think o3-mini actually just performs better with structure responses in general. o1-mini also performs similarly. In contrast, 4o-mini performs the worst it has this entire PSET. I had assumed that it would do quite well as it did great in zero shot, but against my expectations, it appears 4o-mini simply does not do as well with strict guidance. This may be because 4o-mini was created after the usage of Chat-GPT where users do not provide any sort of prompt. Thus, it suggests 4o-mini was trained on mostly zero shot and instructed to perform tasks with no guidance from the user, as that is most similar to user behavior in Chat-GPT. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pw)",
   "language": "python",
   "name": "pw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
