{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S&DS 617 Applied Machine Learning and Causal Inference Research Seminar: Assignment 2\n",
    "\n",
    "**Deadline**\n",
    "\n",
    "Assignment 2 is due Monday, March 24th at 1:30pm. Late work will not be accepted.\n",
    "\n",
    "\n",
    "**Submission**\n",
    "\n",
    "Submit your assignment as a .pdf on Gradescope. On Gradescope, there are 2 assignments, one where you will submit a pdf file and one where you will submit the corresponding .ipynb that generated it. \n",
    "Note: The problems in each homework assignment are numbered.When submitting the pdf on Gradescope, please select the correct pages that correspond to each problem. \n",
    "\n",
    "To produce the .pdf, do the following to preserve the cell structure of the notebook:\n",
    "- Go to \"File\" at the top-left of your Jupyter Notebook\n",
    "- Under \"Download as\", select \"HTML (.html)\"\n",
    "- After the .html has downloaded, open it and then select \"File\" and \"Print\"\n",
    "- From the print window, select the option to save as a .pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "In this exercise, we'll employ different prompt tuning techniques on GSM8k dataset [Link](https://github.com/openai/grade-school-math/tree/master/grade_school_math/data).\n",
    "\n",
    "The GSM8K dataset is an OpenAI-curated collection of 8,500 grade school math problems designed to challenge and evaluate the mathematical reasoning capabilities of language models. It contains approximately 7,500 training and 1,000 test problems that require 2 to 8 steps to solve, using basic arithmetic operations. The dataset aims to diagnose current model limitations in multi-step reasoning and supports advancements in AI's understanding and processing of natural language math problems. It includes both standard problems and a Socratic format with guiding subquestions, along with calculation annotations to aid in accuracy, making it a valuable resource for AI research in natural language processing.\n",
    "\n",
    "Below, we have provided helper functions to load the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import tarfile\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import openai\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "import ast \n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Function to load data from a URL\n",
    "def load_data_from_url(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        data = [json.loads(line) for line in response.iter_lines(decode_unicode=True)]\n",
    "        df = pd.DataFrame(data)\n",
    "        return df\n",
    "    else:\n",
    "        print(f\"Failed to download the file: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Function to get the true solution from JSON file \n",
    "def extract_solution(problem):\n",
    "    \"\"\"\n",
    "    Extracts the final numeric solution from a problem dictionary with 'question' and 'answer' keys.\n",
    "    The answer is expected to contain a '####' token followed by the final numeric solution.\n",
    "    \n",
    "    :param problem: A dictionary with 'question' and 'answer' keys.\n",
    "    :return: The final numeric solution as a string.\n",
    "    \"\"\"\n",
    "    # Split the answer into lines\n",
    "    answer_lines = problem['answer'].split('\\n')\n",
    "    # Look for the line with the '####' token\n",
    "    for line in answer_lines:\n",
    "        if '####' in line:\n",
    "            # Extract the numeric solution that follows the '####' token\n",
    "            solution = line.split('####')[-1].strip()\n",
    "            return solution\n",
    "    # If no solution is found, return None\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 500)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# URLs for the train and test data\n",
    "train_url = \"https://raw.githubusercontent.com/openai/grade-school-math/master/grade_school_math/data/train.jsonl\"\n",
    "test_url = \"https://raw.githubusercontent.com/openai/grade-school-math/master/grade_school_math/data/test.jsonl\"\n",
    "\n",
    "# Load the training data\n",
    "df_train = load_data_from_url(train_url)\n",
    "df_train = df_train.sample(n=1000, random_state=928)\n",
    "\n",
    "# Load the test data\n",
    "df_test = load_data_from_url(test_url)\n",
    "df_test = df_test.sample(n=500, random_state=184)\n",
    "\n",
    "# Display the lengths of the datasets as a check\n",
    "len(df_train), len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question    Taegan goes to a carnival where she wins ticke...\n",
      "answer      If tickets are valued at $3 then in total, Tae...\n",
      "Name: 3103, dtype: object\n"
     ]
    }
   ],
   "source": [
    "sample_row = df_train.iloc[0,] \n",
    "print(sample_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Taegan goes to a carnival where she wins tickets from each of the 5 carnival games and also finds 5 tickets on the floor. Each ticket is worth $3. In total, she has tickets that total a value of $30. If Taegan won an equal number of tickets from each of the games, how many tickets did she win from each game?'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_row['question'] # sample question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the solution\n",
    "extract_solution(sample_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Implement zero shot learning, few shot learning, and chain of thought prompting using gpt-4o. Make a figure or table comparing the accuracy of each on the test set and comment on your results and whether they align with your expectations. Sample at most 1000 observations. Again, remember to start with a smaller subset of your dataset to ensure your implementation is correct before scaling up. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero Shot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from dotenv import load_dotenv\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the OpenAI API key\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Use the API key\n",
    "if openai_api_key:\n",
    "    print(\"OpenAI API Key loaded successfully!\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not found. Please check your .env file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up the OpenAI client\n",
    "client = openai.OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"o1-mini\"\n",
    "labels = df_test.apply(extract_solution, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def zero_shot(txt): \n",
    "    chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": f\"Return a single-number answer only. No explanation. {txt}\"}\n",
    "            ],\n",
    "            model=model_type\n",
    "        )\n",
    "    response = chat_completion.choices[0].message.content\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete!\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "import re\n",
    "\n",
    "def process_question(question):\n",
    "    ans = zero_shot(question)\n",
    "    clean_response = re.sub(r\"```.*?\\n\", \"\", ans).strip(\"```\")\n",
    "    return clean_response\n",
    "\n",
    "gpt_results = []\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:  # 5 parallel threads\n",
    "    gpt_results = list(executor.map(process_question, df_test['question']))\n",
    "\n",
    "print(\"Processing complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test results for  o1-mini  zero-shot:  0.938\n"
     ]
    }
   ],
   "source": [
    "acc_results = accuracy_score(labels, gpt_results)\n",
    "print(\"Test results for \", model_type, \" zero-shot: \", acc_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few Shot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"o1-mini\"\n",
    "prompt = \"\"\"\n",
    "Q: What is 10*5? \n",
    "A: 50 \n",
    "\n",
    "Q: How many miles did Rob drive if he drove for 3 hours at 20 miles per hour? \n",
    "A: 60 \n",
    "\"\"\"\n",
    "\n",
    "def few_shot(txt): \n",
    "    chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": f\"Return a single-number answer only. No explanation. Here are a few examples: {prompt}. Q: {txt}\"}\n",
    "            ],\n",
    "            model=model_type# Specify the model\n",
    "        )\n",
    "    response = chat_completion.choices[0].message.content\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete!\n"
     ]
    }
   ],
   "source": [
    "def process_question(question):\n",
    "    ans = few_shot(question)\n",
    "    clean_response = re.sub(r\"```.*?\\n\", \"\", ans).strip(\"```\")\n",
    "    return clean_response\n",
    "\n",
    "gpt_results = []\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:  # 5 parallel threads\n",
    "    gpt_results = list(executor.map(process_question, df_test['question']))\n",
    "\n",
    "print(\"Processing complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test results for  o1-mini  few-shot:  0.958\n"
     ]
    }
   ],
   "source": [
    "acc_results = accuracy_score(labels, gpt_results)\n",
    "print(\"Test results for \", model_type, \" few-shot: \", acc_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain of Thought "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b)  Explore recent literature for reasoning methods that could enhance the performance of CoT and improve the baseline obtained in a). Then, compare the results with the reasoning models o1 and o3, and discuss your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"o1-mini\"\n",
    "prompt = \"\"\"\n",
    "Question 1: \\n \n",
    "Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now? \\n\n",
    "2 cans * 3 tennis balls = 6 tennis balls \\n \n",
    "6 tennis balls he bought + 5 tennis balls he had originally = 11. \n",
    "\n",
    "Question 2: \\n\n",
    "Q: The concert was scheduled to be on 06/01/1943, but was delayed by one day to today. What is the date 10 days ago in MM/DD/YYYY?  \n",
    "A: One day after 06/01/1943 is 06/02/1943, so today is 06/02/1943. 10 days before today is 05/23/1943. So the answer is 05/23/1943. \n",
    "\"\"\"\n",
    "\n",
    "def cot(txt): \n",
    "    chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": f\"Return a single-number answer only. No explanation. Here are a few examples: {prompt}. Question: {txt}\"}\n",
    "            ],\n",
    "            model=model_type# Specify the model\n",
    "        )\n",
    "    response = chat_completion.choices[0].message.content\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete!\n"
     ]
    }
   ],
   "source": [
    "def process_question(question):\n",
    "    ans = cot(question)\n",
    "    clean_response = re.sub(r\"```.*?\\n\", \"\", ans).strip(\"```\")\n",
    "    return clean_response\n",
    "\n",
    "gpt_results = []\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:  # 5 parallel threads\n",
    "    gpt_results = list(executor.map(process_question, df_test['question']))\n",
    "\n",
    "print(\"Processing complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test results for  o1-mini cot:  0.942\n"
     ]
    }
   ],
   "source": [
    "acc_results = accuracy_score(labels, gpt_results)\n",
    "print(\"Test results for \", model_type, \"cot: \", acc_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model   | Prompting Method | Accuracy |\n",
    "|---------|----------------|----------|\n",
    "| o1-mini | Zero-Shot      | 0.938    |\n",
    "| o1-mini | Few-Shot       | 0.958    |\n",
    "| o1-mini | Chain of Thought       | 0.942  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Perform an ablation study similar to that of Section 3.3 of Wei et al. 2023: [Link](https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf). Comment on your results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pw)",
   "language": "python",
   "name": "pw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
